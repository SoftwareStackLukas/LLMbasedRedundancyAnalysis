{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook the User Stories dependencies are transformt to a graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from support_functions.load_data import load_datasets_with_annotations as loading\n",
    "from support_functions.request_handler import send_requierment_to_chatgpt, StoppedAnswerException\n",
    "from support_functions.save_data import save_to_json_persistent\n",
    "from support_functions.data_transformations import transform_data_id_text\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertion_to_user_story_message(data: pd.DataFrame) -> str:\n",
    "    user_stories: str = \"\"\n",
    "    for idx in range(len(data)):\n",
    "            user_stories += \"id: \" + str(data.iat[idx, 0])\n",
    "            user_stories += \";\\t\"\n",
    "            user_stories += \"description: \" + data.iat[idx, 1]\n",
    "            user_stories += \"\\n\"\n",
    "    return user_stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the User Story Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for #G02#:\n",
      "    ID                                         User Story\n",
      "0  215  As a Data user, I want to have the 12-19-2017 ...\n",
      "1  216  As a UI designer, I want to redesign the Resou...\n",
      "2  217  As a UI designer, I want to report to the Agen...\n",
      "3  218  As a UI designer, I want to move on to round 2...\n",
      "4  220  As a UI designer, I want to move on to round 3...\n",
      "Data for #G03#:\n",
      "    ID                                         User Story\n",
      "0  315  As a Public User, I want to Search for Informa...\n",
      "1  316  As a ProspectiveApplicant, I want to research ...\n",
      "2  317  As an Applicant, I want to Request PreApplicat...\n",
      "3  318  As a Customer, I want to Create a Customer Por...\n",
      "4  319  As an Applicant, I want to Submit Application,...\n",
      "Data for #G04#:\n",
      "    ID                                         User Story\n",
      "0  164  As a user, I want to click on the address, so ...\n",
      "1  165  As a user, I want to be able to anonymously vi...\n",
      "2  166  As a user, I want to be able to enter my zip c...\n",
      "3  167  As a user, I want to be able to get the hours ...\n",
      "4  168  As a user, I want to have a flexible pick up t...\n",
      "Data for #G05#:\n",
      "    ID                                         User Story\n",
      "0  373  As a Data Publishing User, I want to be able t...\n",
      "1  374  As a Data Publishing User, I want to be able t...\n",
      "2  375  As a Data Publishing User, I want to be able t...\n",
      "3  376  As a Platform Administrator, I want to be able...\n",
      "4  377  As a Platform Administrator, I want to have a ...\n",
      "Data for #G08#:\n",
      "    ID                                         User Story\n",
      "0  426  As a Developer, I want to get a Data Package i...\n",
      "1  427  As a Researcher, I want to get a Data Package ...\n",
      "2  428  As a Publisher, I want to add type information...\n",
      "3  429  As a Publisher, I want to be able to provide a...\n",
      "4  430  As a Researcher, I want to be able to save new...\n",
      "Data for #G11#:\n",
      "    ID                                         User Story\n",
      "0  590  As a team member, I want to see the first iter...\n",
      "1  591  As a developer, I want to have the subdomain b...\n",
      "2  592  As a stakeholder, I want to have a decision on...\n",
      "3  593  As an NSF employee, I want to understand the p...\n",
      "4  594  As a stakeholder, I want to see the results of...\n",
      "Data for #G12#:\n",
      "    ID                                         User Story\n",
      "0  610  As a camp administrator, I want to be able to ...\n",
      "1  611  As a camp administrator, I want to be able to ...\n",
      "2  612  As a camp administrator, I want to be able to ...\n",
      "3  613  As a camp administrator, I want to be able to ...\n",
      "4  614  As a camp administrator, I want to be able to ...\n",
      "Data for #G13#:\n",
      "    ID                                         User Story\n",
      "0  664  As a moderator, I want to create a new game by...\n",
      "1  665  As a moderator, I want to invite estimators by...\n",
      "2  666  As a moderator, I want to start a round by ent...\n",
      "3  667  As a moderator, I want to see all items we try...\n",
      "4  668  As a moderator, I want to select an item to be...\n",
      "Data for #G14#:\n",
      "    ID                                         User Story\n",
      "0  717  As a Publisher, I want to publish a dataset, s...\n",
      "1  718  As a Publisher, I want to publish a dataset, s...\n",
      "2  719  As a Publisher, I want to sign up for an accou...\n",
      "3  720  As a Visitor, I want to sign up via github or ...\n",
      "4  721  As a Publisher, I want to know what do next af...\n",
      "Data for #G16#:\n",
      "    ID                                         User Story\n",
      "0  784  As a collection curator, I want to set a date ...\n",
      "1  785  As a collection curator, I want to see the sch...\n",
      "2  786  As a repository manager, I want to track who i...\n",
      "3  787  As a repository manager, I want to be able t r...\n",
      "4  788  As a collection curator, I want to map an item...\n",
      "Data for #G17#:\n",
      "    ID                                         User Story\n",
      "0  852  As an app developer, I want to include the cod...\n",
      "1  853  As an app developer, I want to deploy a new ve...\n",
      "2  854  As an app developer, I want to deploy a new ve...\n",
      "3  855  As an app developer, I want to explore a datas...\n",
      "4  856  As an app developer, I want to ensure that whe...\n",
      "Data for #G18#:\n",
      "    ID                                         User Story\n",
      "0  916  As a system administrator, I want to run a scr...\n",
      "1  917  As a system administrator, I want to run a scr...\n",
      "2  918  As a release engineer, I want to call a script...\n",
      "3  919  As an investigator, I want to view the accepta...\n",
      "4  920  As an MRI operator, I want to digitally record...\n",
      "Data for #G19#:\n",
      "     ID                                         User Story\n",
      "0  1018  As an OlderPerson, I want to know exactly what...\n",
      "1  1019  As an OlderPerson, I want to receive informati...\n",
      "2  1020  As an OlderPerson, I want to use only well-vis...\n",
      "3  1021  As an OlderPerson, I want to use ALFRED as muc...\n",
      "4  1022  As an OlderPerson, I want to always keep contr...\n",
      "Data for #G21#:\n",
      "     ID                                         User Story\n",
      "0  1156  As a anonymoususer, I want to view a list of s...\n",
      "1  1157  As a anonymoususer, I want to view a list of u...\n",
      "2  1158  As a trainingcoordinator, I want to email all ...\n",
      "3  1159  As a attendee, I want to have a very clear map...\n",
      "4  1160  As a trainer, I want to edit my training node ...\n",
      "Data for #G22#:\n",
      "     ID                                         User Story\n",
      "0  1225  As a PI, I want to properly record all metadat...\n",
      "1  1226  As a researcher, I want to indicate where my d...\n",
      "2  1227  As a researcher, I want to import metadata tha...\n",
      "3  1228  As a data manager, I want to know how the data...\n",
      "4  1229  As a repository owner, I want to be able to ch...\n",
      "Data for #G23#:\n",
      "     ID                                         User Story\n",
      "0  1308  As a researcher, I want to be able to view a f...\n",
      "1  1309  As an Application Administrator, I want to be ...\n",
      "2  1310  As a user, I want backend changes for managing...\n",
      "3  1311  As an Archivist, I want to import EAD files th...\n",
      "4  1312  As an Archivist, I want to assign a component ...\n",
      "Data for #G24#:\n",
      "     ID                                         User Story\n",
      "0  1365  As a depositor, I want to deposit and maintain...\n",
      "1  1366  As a depositor, I want to have a user interfac...\n",
      "2  1367  As a depositor, I want to deposit and maintain...\n",
      "3  1368  As a depositor, I want to deposit and maintain...\n",
      "4  1369  As a depositor, I want to deposit the files th...\n",
      "Data for #G25#:\n",
      "     ID                                         User Story\n",
      "0  1418  As a repository manager, I want to know all th...\n",
      "1  1419  As a repository manager, I want to be able to ...\n",
      "2  1420  As a repository manager, I want to constrain c...\n",
      "3  1421  As a user, I want to list all the collections ...\n",
      "4  1422  As a user, I want to be able to contact reposi...\n",
      "Data for #G26#:\n",
      "     ID                                         User Story\n",
      "0  1518  As an archivist, I want to apply a license or ...\n",
      "1  1519  As an archivist, I want to mark individual fil...\n",
      "2  1520  As an archivist, I want to mark groups of file...\n",
      "3  1521  As an archivist, I want to restrict a file fro...\n",
      "4  1522  As an archivist, I want to restrict a group of...\n",
      "Data for #G27#:\n",
      "     ID                                         User Story\n",
      "0  1618  As a faculty member, I want to access a collec...\n",
      "1  1619  As a faculty member, I want to direct students...\n",
      "2  1620  As a Cornell faculty member, I want to upload ...\n",
      "3  1621  As a patron, I want to know which reports/data...\n",
      "4  1622  As a patron, I want to select/deselect reports...\n",
      "Data for #G28#:\n",
      "     ID                                         User Story\n",
      "0  1733  As a user, I want to search and discover music...\n",
      "1  1734  As a user, I want to search and discover music...\n",
      "2  1735  As a user, I want to search and discover music...\n",
      "3  1736  As a user, I want to search and discover music...\n",
      "4  1737  As a user, I want to search and discover music...\n"
     ]
    }
   ],
   "source": [
    "datasets: dict[str, list] = loading()\n",
    "\n",
    "data_g02 = transform_data_id_text(datasets[\"#G02#\"])\n",
    "print(\"Data for #G02#:\")\n",
    "print(data_g02.head())\n",
    "\n",
    "data_g03 = transform_data_id_text(datasets[\"#G03#\"])\n",
    "print(\"Data for #G03#:\")\n",
    "print(data_g03.head())\n",
    "\n",
    "data_g04 = transform_data_id_text(datasets[\"#G04#\"])\n",
    "print(\"Data for #G04#:\")\n",
    "print(data_g04.head())\n",
    "\n",
    "data_g05 = transform_data_id_text(datasets[\"#G05#\"])\n",
    "print(\"Data for #G05#:\")\n",
    "print(data_g05.head())\n",
    "\n",
    "data_g08 = transform_data_id_text(datasets[\"#G08#\"])\n",
    "print(\"Data for #G08#:\")\n",
    "print(data_g08.head())\n",
    "\n",
    "data_g11 = transform_data_id_text(datasets[\"#G11#\"])\n",
    "print(\"Data for #G11#:\")\n",
    "print(data_g11.head())\n",
    "\n",
    "data_g12 = transform_data_id_text(datasets[\"#G12#\"])\n",
    "print(\"Data for #G12#:\")\n",
    "print(data_g12.head())\n",
    "\n",
    "data_g13 = transform_data_id_text(datasets[\"#G13#\"])\n",
    "print(\"Data for #G13#:\")\n",
    "print(data_g13.head())\n",
    "\n",
    "data_g14 = transform_data_id_text(datasets[\"#G14#\"])\n",
    "print(\"Data for #G14#:\")\n",
    "print(data_g14.head())\n",
    "\n",
    "data_g16 = transform_data_id_text(datasets[\"#G16#\"])\n",
    "print(\"Data for #G16#:\")\n",
    "print(data_g16.head())\n",
    "\n",
    "data_g17 = transform_data_id_text(datasets[\"#G17#\"])\n",
    "print(\"Data for #G17#:\")\n",
    "print(data_g17.head())\n",
    "\n",
    "data_g18 = transform_data_id_text(datasets[\"#G18#\"])\n",
    "print(\"Data for #G18#:\")\n",
    "print(data_g18.head())\n",
    "\n",
    "data_g19 = transform_data_id_text(datasets[\"#G19#\"])\n",
    "print(\"Data for #G19#:\")\n",
    "print(data_g19.head())\n",
    "\n",
    "data_g21 = transform_data_id_text(datasets[\"#G21#\"])\n",
    "print(\"Data for #G21#:\")\n",
    "print(data_g21.head())\n",
    "\n",
    "data_g22 = transform_data_id_text(datasets[\"#G22#\"])\n",
    "print(\"Data for #G22#:\")\n",
    "print(data_g22.head())\n",
    "\n",
    "data_g23 = transform_data_id_text(datasets[\"#G23#\"])\n",
    "print(\"Data for #G23#:\")\n",
    "print(data_g23.head())\n",
    "\n",
    "data_g24 = transform_data_id_text(datasets[\"#G24#\"])\n",
    "print(\"Data for #G24#:\")\n",
    "print(data_g24.head())\n",
    "\n",
    "data_g25 = transform_data_id_text(datasets[\"#G25#\"])\n",
    "print(\"Data for #G25#:\")\n",
    "print(data_g25.head())\n",
    "\n",
    "data_g26 = transform_data_id_text(datasets[\"#G26#\"])\n",
    "print(\"Data for #G26#:\")\n",
    "print(data_g26.head())\n",
    "\n",
    "data_g27 = transform_data_id_text(datasets[\"#G27#\"])\n",
    "print(\"Data for #G27#:\")\n",
    "print(data_g27.head())\n",
    "\n",
    "data_g28 = transform_data_id_text(datasets[\"#G28#\"])\n",
    "print(\"Data for #G28#:\")\n",
    "print(data_g28.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepairing the connection to ChatGPT --> Maybe I have to junk it that the output input and output size are not to big by iterativly going for just the outgoind connections and then combine the incoming connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_structure_template = '''\n",
    "    [\n",
    "        {\n",
    "            \"id\": \"UserStoryID1\",\n",
    "            \"user_story_description\": \"UserStoryDescription1\",\n",
    "            \"out_going_connections\": [\n",
    "                {\"id\": \"ConnectedUserStoryID1\", \"connection_weight\": \"ConnectionWeight1\"},\n",
    "                {\"id\": \"ConnectedUserStoryID2\", \"connection_weight\": \"ConnectionWeight2\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"UserStoryID2\",\n",
    "            \"user_story_description\": \"UserStoryDescription2\",\n",
    "            \"out_going_connections\": [\n",
    "                {\"id\": \"ConnectedUserStoryID3\", \"connection_weight\": \"ConnectionWeight3\"},\n",
    "                {\"id\": \"ConnectedUserStoryID4\", \"connection_weight\": \"ConnectionWeight4\"}\n",
    "            ]\n",
    "        },\n",
    "        // Add all the remaining user stories here...\n",
    "    ]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "message: list[dict] = []\n",
    "#{\"role\":\"\", \"message\":\"\"}\n",
    "\n",
    "message.append({\"role\": \"user\", \"content\": \"I want you to be a Requirements Engineer who generates a dependency graph based on User Stories.\"})\n",
    "message.append({\"role\": \"system\", \"content\": \"Yes, of course. I can be your Requirements Engineer. Please provide me with how I should build this dependency graph.\"})\n",
    "\n",
    "# Defining a model-based approach based on graphs\n",
    "def_model: str = '''In model-driven dependency analyses (MDDA), domain-specific models are used to define the structure of data objects. \n",
    "In MDDA, we use graph-like structures to represent our data objects as well as their interconnections.'''\n",
    "message.append({\"role\": \"user\", \"content\": \"First of all, we want to set the context in which we work. We want to follow a model-driven approach. Thus, consider the following definition: \" + def_model})\n",
    "message.append({\"role\": \"system\", \"content\": \"Fine, I can consider this context. Could you define for me what you consider as a graph to improve the results?\"})\n",
    "\n",
    "# Defining the graph\n",
    "def_dependency_graph: str = \"A dependency graph is a graph that illustrates the dependencies of a User Story. \"\n",
    "def_graph: str = '''A directed typed graph contains a variety of nodes which can or cannot be connected by edges. A node is an object representation like an entity of a recipe e.g., sugar, \n",
    "a class in Java, or a user story. Edges represent the interconnections between nodes. Thus, an edge has a source node and a target node. Therefore, an edge is directed. \n",
    "A \"node A\" can have multiple or no edges to a \"node B\". Furthermore, nodes and edges can have properties. Properties are key-value pairs associated with nodes and edges. \n",
    "They contain additional information about the nodes they are attached to.'''\n",
    "message.append({\"role\": \"user\", \"content\": \"Yes, I can define it for you. \" + def_dependency_graph + \"Consider the following graph definition as well: \" + def_graph})\n",
    "message.append({\"role\": \"system\", \"content\": \"Okay, I understand. I will consider this definition. Can you also define for me how to map a directed typed graph to the user stories?\"})\n",
    "\n",
    "# Defining how User Stories shall be represented in a directed dependency graph\n",
    "def_graph_user_stories: str = '''In our MDDA approach, we want to represent User Stories as nodes of a graph. Hence, a node represents exactly one user story and has a property which stores its description as well as a property id which stores the identifier of a User Story.\n",
    "Edges represent the connections of the different nodes. Not all user stories are connected by edges, but other user stories could have multiple connections to other user stories. Therefore, each edge that connects two user stories has a strength weight from 1 to 10, representing how connected two user stories are.\n",
    "It has to be considered that we exclude the strength weight 0 as it would mean that there is no relation between two user stories. \n",
    "Furthermore, an edge between two User Stories is directed. This means one User Story is always before the other User Story. In the case that two User Stories require each other, two directed edges connect the nodes.'''\n",
    "message.append({\"role\": \"user\", \"content\": \"For sure. The mapping is as follows: \" + def_graph_user_stories})\n",
    "\n",
    "# Definition of Clustering approach\n",
    "def_clustering_approach: str = '''The clustering approach is as follows:\n",
    "1.) First, find strongly correlated clusters of the User Stories and think of these Clusters as graphs. Cluster by the following: 1.) Benefit (the outcome of the action), 2.) Entity (a noun representing an element involved in the system), 3.) Action (an action (verb) used by a Persona to act on an Entity; Adjectives are included in the action quickly upload and not only upload; Sometimes a verb may be used to describe an entity. In such a case, it is not an action but a descriptor. For example, the updated data), 4.) Persona (the noun used to name each persona involved in this story; Adjectives are included in the persona Public User and not only User)  \n",
    "2.) Secondly, label each cluster internally.\n",
    "4.) Convert this cluster structure into separate (sub-)graphs. Therefor, consider that a User Story is a node and has properties like the id (identification number of a User Story) and a description (textual description of the user story).\n",
    "3.) Thirdly, search for connections between the nodes (User Stories) based on their description and not their id. Create fitting directed edges between the nodes. Assign to each edge a weight strength between 1 and 10, where 1 is a weak connection and 10 is a strong connection.\n",
    "4.) Fourth, search connections between the nodes of the different subgraphs and connect them as well, and assign a weight strength to each edge as a property.'''\n",
    "message.append({\"role\": \"user\", \"content\": \"Moreover, consider the following clustering approach to build the dependency graph: \" + def_clustering_approach})\n",
    "\n",
    "# Defining the return structure\n",
    "def_return_structure: str = '''The generated graph with its subgraphs have now to be converted to a JSON format. Please, convert the graph into the following template JSON format: ''' + graph_structure_template\n",
    "message.append({\"role\": \"user\", \"content\": def_return_structure})\n",
    "message.append({\"role\": \"system\", \"content\": \"I will generate the clustering for the graph as described. Afterwards, I will return the graph based on the defined JSON structure.\"})\n",
    "message.append({\"role\": \"system\", \"content\": \"Please, provide me with the User Stories.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of Graph for federal funding User Stories\n",
    "\n",
    "Find a libary which calculates the tokens and then depending on this tokenise the request. A online tool is: https://www.quizrise.com/token-counter to calculate it.\n",
    "On this website the max. tokens are represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_g02: list[dict] = message.copy()\n",
    "user_stories_g02: str = convertion_to_user_story_message(data_g02)\n",
    "message_g02.append({\"role\":\"user\", \"content\": \"Use the folowing user stories:\\n\" + user_stories_g02 +\"\\n Please, generated the graph as described before.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user; I want you to be a Requirements Engineer who generates a dependency graph based on User Stories.\n",
      "system; Yes, of course. I can be your Requirements Engineer. Please provide me with how I should build this dependency graph.\n",
      "user; First of all, we want to set the context in which we work. We want to follow a model-driven approach. Thus, consider the following definition: In model-driven dependency analyses (MDDA), domain-specific models are used to define the structure of data objects. \n",
      "In MDDA, we use graph-like structures to represent our data objects as well as their interconnections.\n",
      "system; Fine, I can consider this context. Could you define for me what you consider as a graph to improve the results?\n",
      "user; Yes, I can define it for you. A dependency graph is a graph that illustrates the dependencies of a User Story. Consider the following graph definition as well: A directed typed graph contains a variety of nodes which can or cannot be connected by edges. A node is an object representation like an entity of a recipe e.g., sugar, \n",
      "a class in Java, or a user story. Edges represent the interconnections between nodes. Thus, an edge has a source node and a target node. Therefore, an edge is directed. \n",
      "A \"node A\" can have multiple or no edges to a \"node B\". Furthermore, nodes and edges can have properties. Properties are key-value pairs associated with nodes and edges. \n",
      "They contain additional information about the nodes they are attached to.\n",
      "system; Okay, I understand. I will consider this definition. Can you also define for me how to map a directed typed graph to the user stories?\n",
      "user; For sure. The mapping is as follows: In our MDDA approach, we want to represent User Stories as nodes of a graph. Hence, a node represents exactly one user story and has a property which stores its description as well as a property id which stores the identifier of a User Story.\n",
      "Edges represent the connections of the different nodes. Not all user stories are connected by edges, but other user stories could have multiple connections to other user stories. Therefore, each edge that connects two user stories has a strength weight from 1 to 10, representing how connected two user stories are.\n",
      "It has to be considered that we exclude the strength weight 0 as it would mean that there is no relation between two user stories. \n",
      "Furthermore, an edge between two User Stories is directed. This means one User Story is always before the other User Story. In the case that two User Stories require each other, two directed edges connect the nodes.\n",
      "user; Moreover, consider the following clustering approach to build the dependency graph: The clustering approach is as follows:\n",
      "1.) First, find strongly correlated clusters of the User Stories and think of these Clusters as graphs. Cluster by the following: 1.) Benefit (the outcome of the action), 2.) Entity (a noun representing an element involved in the system), 3.) Action (an action (verb) used by a Persona to act on an Entity; Adjectives are included in the action quickly upload and not only upload; Sometimes a verb may be used to describe an entity. In such a case, it is not an action but a descriptor. For example, the updated data), 4.) Persona (the noun used to name each persona involved in this story; Adjectives are included in the persona Public User and not only User)  \n",
      "2.) Secondly, label each cluster internally.\n",
      "4.) Convert this cluster structure into separate (sub-)graphs. Therefor, consider that a User Story is a node and has properties like the id (identification number of a User Story) and a description (textual description of the user story).\n",
      "3.) Thirdly, search for connections between the nodes (User Stories) based on their description and not their id. Create fitting directed edges between the nodes. Assign to each edge a weight strength between 1 and 10, where 1 is a weak connection and 10 is a strong connection.\n",
      "4.) Fourth, search connections between the nodes of the different subgraphs and connect them as well, and assign a weight strength to each edge as a property.\n",
      "user; The generated graph with its subgraphs have now to be converted to a JSON format. Please, convert the graph into the following template JSON format: \n",
      "    [\n",
      "        {\n",
      "            \"ID\": \"UserStoryID1\",\n",
      "            \"User_Story_Description\": \"UserStoryDescription1\",\n",
      "            \"OUT_GOING_CONNECTIONS\": [\n",
      "                {\"ID\": \"ConnectedUserStoryID1\", \"connection_weight\": \"ConnectionWeight1\"},\n",
      "                {\"ID\": \"ConnectedUserStoryID2\", \"connection_weight\": \"ConnectionWeight2\"}\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"ID\": \"UserStoryID2\",\n",
      "            \"User_Story_Description\": \"UserStoryDescription2\",\n",
      "            \"OUT_GOING_CONNECTIONS\": [\n",
      "                {\"ID\": \"ConnectedUserStoryID3\", \"connection_weight\": \"ConnectionWeight3\"},\n",
      "                {\"ID\": \"ConnectedUserStoryID4\", \"connection_weight\": \"ConnectionWeight4\"}\n",
      "            ]\n",
      "        },\n",
      "        // Add all the remaining user stories here...\n",
      "    ]\n",
      "\n",
      "system; I will generate the clustering for the graph as described. Afterwards, I will return the graph based on the defined JSON structure.\n",
      "system; Please, provide me with the User Stories.\n",
      "user; Use the folowing user stories:\n",
      "id: 215;\tdescription: As a Data user, I want to have the 12-19-2017 deletions processed.\n",
      "id: 216;\tdescription: As a UI designer, I want to redesign the Resources page, so that it matches the new Broker design styles.\n",
      "id: 217;\tdescription: As a UI designer, I want to report to the Agencies about user testing, so that they are aware of their contributions to making Broker a better UX.\n",
      "id: 218;\tdescription: As a UI designer, I want to move on to round 2 of DABS or FABS landing page edits, so that I can get approvals from leadership.\n",
      "id: 220;\tdescription: As a UI designer, I want to move on to round 3 of the Help page edits, so that I can get approvals from leadership.\n",
      "id: 221;\tdescription: As a Developer , I want to be able to log better, so that I can troubleshoot issues with particular submissions and functions.\n",
      "id: 222;\tdescription: As a Developer, I want to add the updates on a FABS submission to be modified when the publishStatus changes, so that I know when the status of the submission has changed.\n",
      "id: 223;\tdescription: As a DevOps engineer, I want New Relic to provide useful data across all applications.\n",
      "id: 224;\tdescription: As a UI designer,  I want to move on to round 2 of the Help page edits, so that I can get approvals from leadership.\n",
      "id: 225;\tdescription: As a UI designer, I want to move on to round 2 of Homepage edits, so that I can get approvals from leadership.\n",
      "id: 226;\tdescription: As a Broker user, I want to Upload and Validate the error message to have accurate text.\n",
      "id: 227;\tdescription: As a Broker user, I want the D1 file generation to be synced with the FPDS data load, so that I don't have to regenerate a file if no data has been updated.\n",
      "id: 228;\tdescription: As a Website user, I want to access published FABS files, so that I can see the new files as they come in.\n",
      "id: 229;\tdescription: As an owner, I want to be sure that USAspending only send grant records to my system.\n",
      "id: 230;\tdescription: As a Developer, I want to update the Broker validation rule table to account for the rule updates in DB-2213.\n",
      "id: 231;\tdescription: As a Developer, I want to add the GTAS window data to the database, so that I can ensure the site is locked down during the GTAS submission period.\n",
      "id: 232;\tdescription: As a Developer, I want D Files generation requests to be managed and cached, so that duplicate requests do not cause performance issues.\n",
      "id: 234;\tdescription: As an Agency user, I want to be able to include a large number of flexfields without performance impact.\n",
      "id: 235;\tdescription: As a Broker user, I want  to help create content mockups, so that I can submit my data efficiently.\n",
      "id: 236;\tdescription: As a UI designer, I want to track the issues that come up in Tech Thursday, so that I know what to test and what want s to be fixed.\n",
      "id: 237;\tdescription: As an Owner, I want to create a user testing summary from the UI SME, so that I can know what UI improvements we will follow through on.\n",
      "id: 238;\tdescription: As a UI designer, I want to begin user testing, so that I can validate stakeholder UI improvement requests.\n",
      "id: 239;\tdescription: As a UI designer, I want to schedule user testing, so that I can give the testers advanced notice to ensure buy-in.\n",
      "id: 240;\tdescription: As an Owner, I want to design a schedule from the UI SME, so that I know the potential timeline of the UI improvements wanted.\n",
      "id: 241;\tdescription: As an Owner, I want to design an audit from the UI SME, so that I know the potential scope of the UI improvements want ed.\n",
      "id: 242;\tdescription: As a Developer, I want to prevent users from double publishing FABS submissions after refreshing, so that there are no duplicates.\n",
      "id: 243;\tdescription: As an data user, I want to receive updates to FABS records.\n",
      "id: 245;\tdescription: As a Developer , I want to update the FABS sample file to remove FundingAgencyCode after FABS is updated to no longer require the header.\n",
      "id: 246;\tdescription: As an agency user, I want to ensure that deleted FSRS records are not included in submissions.\n",
      "id: 247;\tdescription: As a website user, I want to see updated financial assistance data daily.\n",
      "id: 248;\tdescription: As a user, I want the publish button in FABS to deactivate after I click it while the derivations are happening, so that I cannot click it multiple times for the same submission.\n",
      "id: 249;\tdescription: As a Developer , I want to ensure that attempts to correct or delete non-existent records don't create new published data.\n",
      "id: 250;\tdescription: As an Owner, I want to reset the environment to only take Staging MAX permissions, so that I can ensure that the FABS testers no longer have access.\n",
      "id: 251;\tdescription: As a user, I want the flexfields in my submission file to appear in the warning and error files when the only error is a missing required element.\n",
      "id: 252;\tdescription: As a user, I want to have accurate and complete data related to PPoPCode and PPoPCongressionalDistrict.\n",
      "id: 253;\tdescription: As an agency user, I want the FABS validation rules to accept zero and blank for loan records.\n",
      "id: 254;\tdescription: As an Agency user, I want FABS deployed into production, so I can submit my Financial Assistance data.\n",
      "id: 255;\tdescription: As a Developer , I want to clarify to users what exactly is triggering the CFDA error code in each case.\n",
      "id: 256;\tdescription: As an agency user, I want to be confident that the data coming from SAM is complete.\n",
      "id: 257;\tdescription: As a Developer , I want my domain models to be indexed properly, so that I can get validation results back in a reasonable amount of time.\n",
      "id: 258;\tdescription: As an agency user, I want the FABS validation rules to accept zero and blank for non-loan records.\n",
      "id: 259;\tdescription: As a broker team member, I want to make some updates to the SQL codes for clarity.\n",
      "id: 260;\tdescription: As an agency user, I want to have all derived data elements derived properly.\n",
      "id: 261;\tdescription: As a broker team member, I want to add the 00***** and 00FORGN PPoPCode cases to the derivation logic.\n",
      "id: 262;\tdescription: As a data user, I want to see the office names derived from office codes, so that I can have appropriate context for understanding them.\n",
      "id: 263;\tdescription: As a broker user, I want the historical FABS loader to derive fields, so that my agency codes are correct in the PublishedAwardFinancialAssistance table.\n",
      "id: 264;\tdescription: As a broker team member, I want to ensure the Broker resources, validations, and P&P pages are updated appropriately for the launch of FABS and DAIMS v1.1.\n",
      "id: 265;\tdescription: As a Developer, I want the data loaded from historical FABS to include the FREC derivations, so that I can have consistent FREC data for USASpending.gov.\n",
      "id: 266;\tdescription: As a user, I don't want to see NASA grants displayed as contracts.\n",
      "id: 267;\tdescription: As a user, I want the DUNS validations to accept records whose ActionTypes are B, C, or D and the DUNS is registered in SAM, even though it may have expired.\n",
      "id: 268;\tdescription: As a user, I want the DUNS validations to accept records whose ActionDates are before the current registration date in SAM, but after the initial registration date.\n",
      "id: 269;\tdescription: As a broker team member, I want to derive FundingAgencyCode, so that the data quality and completeness improves.\n",
      "id: 270;\tdescription: As an agency user, I want the maximum length allowed for LegalEntityAddressLine3 to match Schema v1.1.\n",
      "id: 271;\tdescription: As an agency user, I want to use the schema v1.1 headers in my FABS file.\n",
      "id: 272;\tdescription: As a agency user, I want to map the FederalActionObligation properly to the Atom Feed.\n",
      "id: 273;\tdescription: As a Broker user, I want to have PPoPZIP+4 work the same as the Legal Entity ZIP validations.\n",
      "id: 274;\tdescription: As a FABS user, I want to link the SAMPLE FILE on the \"What you want  to submit\" dialog to point to the correct file, so that I have an accurate reference for my agency submissions.\n",
      "id: 275;\tdescription: As an Agency user, I want FPDS data to be up-to-date daily.\n",
      "id: 276;\tdescription: As a user, I want to access the raw agency published files from FABS via USAspending.\n",
      "id: 277;\tdescription: As a Developer , I want to determine how agencies will generate and validate D Files from FABS and FPDS data.\n",
      "id: 278;\tdescription: As a user, I want to generate and validate D Files from FABS and FPDS data.\n",
      "id: 279;\tdescription: As an Agency user, I want the header information box to show updated date AND time, so that I know when it was updated.\n",
      "id: 280;\tdescription: As an Agency user, I want to receive a more helpful file-level error when I upload a file with the wrong extension.\n",
      "id: 281;\tdescription: As a tester, I want to have access to test features in environments other than Staging, so that I can test any nonProd feature in any environment.\n",
      "id: 282;\tdescription: As a FABS user, I want to submission errors to accurately represent FABS errors, so that I know why my submission didn't work.\n",
      "id: 283;\tdescription: As a FABS user, I want the frontend URLs to more accurately reflect the page I'm accessing, so that I'm not confused.\n",
      "id: 284;\tdescription: As an Agency user, I want all historical Financial Assistance data loaded for FABS go-live.\n",
      "id: 285;\tdescription: As a Developer , I want the historical FPDS data loader to include both extracted historical data and FPDS feed data.\n",
      "id: 286;\tdescription: As an Agency user, I want historical FPDS data loaded.\n",
      "id: 287;\tdescription: As an Agency user, I want to accurately see who created a submission, so that I'm not confused about who last updated a submission.\n",
      "id: 288;\tdescription: As an agency user, I want to get File F in the correct format.\n",
      "id: 289;\tdescription: As an Agency user, I want to better understand my file-level errors.\n",
      "id: 290;\tdescription: As a Developer , I want to provide FABS groups that function under the FREC paradigm.\n",
      "id: 291;\tdescription: As a tester, I want to ensure that FABS is deriving fields properly through a robust test file plus a follow up check.\n",
      "id: 292;\tdescription: As an owner, I only want zero-padded fields, so that I can justify padding.\n",
      "id: 293;\tdescription: As a Broker user, I want to submit records for individual recipients without receiving a DUNS error.\n",
      "id: 294;\tdescription: As a user, I want more information about how many rows will be published prior to deciding whether to publish.\n",
      "id: 295;\tdescription: As a Developer, I want to prevent duplicate transactions from being published and deal with the time gap between validation and the publishing decision.\n",
      "id: 296;\tdescription: As a FABS user, I want to submit a citywide as a PPoPZIP and pass validations.\n",
      "id: 297;\tdescription: As a Broker user, I want to have updated error codes that accurately reflect the logic and provide enough information, so that I can fix my submission.\n",
      "id: 298;\tdescription: As an agency user, I want to leave off the last 4 digits of the ZIP without an error, so that I can complete my submissions.\n",
      "id: 299;\tdescription: As a FABS user, I want to make sure the historical data includes all necessary columns, so that the information in the database is correct.\n",
      "id: 300;\tdescription: As a data user, I want to access two additional fields from the FPDS data pull.\n",
      "id: 301;\tdescription: As a FABS user, I want additional helpful info in the submission dashboard, so that I can better manage submissions and IG requests.\n",
      "id: 302;\tdescription: As a FABS user, I want to download the uploaded FABS file, so that I can get the uploaded file.\n",
      "id: 303;\tdescription: As a Developer I want to quickly access Broker application data, so that I can investigate issues.\n",
      "id: 304;\tdescription: As a Developer , I want to determine the best way to load historical FPDS data, so that I can load all FPDS data since 2007.\n",
      "id: 305;\tdescription: As a FABS user, I want the language on FABS pages to be appropriate for me, so that I am not confused.\n",
      "id: 306;\tdescription: As a FABS user, I do not want  DABS banner messages and vice versa, so that I have the appropriate information for my application.\n",
      "id: 307;\tdescription: As a FABS user, I want to have read-only access to DABS, so that I can view DABS pages without wanting two sets of permissions.\n",
      "id: 308;\tdescription: As a FABS user, I want to have my validations run in a reasonable amount of time.\n",
      "id: 309;\tdescription: As a FABS user, I want to see correct status labels on the Submission Dashboard, so that I can quickly see my submission history.\n",
      "id: 310;\tdescription: As an agency user, I want to know when the submission periods start and end, so that I know when the submission starts and ends.\n",
      "id: 311;\tdescription: As an agency user, I want a landing page to navigate to either FABS or DABS pages, so that I can access both sides of the site.\n",
      "id: 312;\tdescription: As an agency user, I want to submit my data elements surrounded by quotation marks, so that Excel won't strip off leading and trailing zeroes.\n",
      "\n",
      " Please, generated the graph as described before.\n"
     ]
    }
   ],
   "source": [
    "for mes in message_g02:\n",
    "    print (mes[\"role\"] + \"; \" + mes[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "results: dict = {}\n",
    "try:\n",
    "    graph_g02: str = send_requierment_to_chatgpt(message_g02)\n",
    "    results[\"#G02#\"] = json.loads(graph_g02)\n",
    "except StoppedAnswerException as e:\n",
    "    results[\"#G02#_Exception\"] = json.loads({\"Reason\" : \"Not stopped exception from ChatGPT Endpoint\"})\n",
    "except ValueError as e:\n",
    "    results[\"#G02#_Exception\"] = json.loads({\"Reason\" : \"ValueError\"})\n",
    "\n",
    "save_to_json_persistent(\"dependency_graph\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of Graph for loudoun User Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT tells me it can not do it. However it recommands the following approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate meaningful relationships or dependencies between the user stories using natural language processing or any similar analysis would require a bit more setup and complexity than can be handled directly here. However, I can guide you on how to start this process using Python, employing basic NLP techniques to analyze and possibly identify relationships between user stories.\n",
    "\n",
    "Approach\n",
    "Extract Keywords: Identify key nouns and verbs from each user story that might suggest common themes or functionalities.\n",
    "\n",
    "Text Similarity: Measure the similarity between the texts of different user stories to find potential connections.\n",
    "\n",
    "Manual Rules: Implement simple rules based on common keywords or phrases that suggest a relationship (e.g., shared terms like \"UI design\", \"developer\", etc.).\n",
    "\n",
    "Example Python Code Using NLP\n",
    "You can use libraries like nltk for basic NLP tasks and sklearn for calculating text similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Ensure you have the necessary NLTK datasets\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# # Example user stories\n",
    "# user_stories = {\n",
    "#     215: \"As a Data user, I want to have the 12-19-2017 deletions processed.\",\n",
    "#     216: \"As a UI designer, I want to redesign the Resources page, so that it matches the new Broker design styles.\",\n",
    "#     217: \"As a UI designer, I want to report to the Agencies about user testing, so that they are aware of their contributions to making Broker a better UX.\",\n",
    "#     # Add more user stories here\n",
    "# }\n",
    "\n",
    "# # Extracting nouns and verbs to analyze\n",
    "# def extract_key_words(text):\n",
    "#     words = nltk.word_tokenize(text)\n",
    "#     pos_tags = nltk.pos_tag(words)\n",
    "#     # Extracting nouns (NN) and verbs (VB)\n",
    "#     key_words = [word for word, tag in pos_tags if 'NN' in tag or 'VB' in tag]\n",
    "#     return ' '.join(key_words)\n",
    "\n",
    "# # Processing user stories\n",
    "# processed_stories = {id: extract_key_words(desc) for id, desc in user_stories.items()}\n",
    "\n",
    "# # Vectorizing stories for similarity analysis\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform(processed_stories.values())\n",
    "# similarities = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# # Print similarities\n",
    "# for i, (id1, desc1) in enumerate(processed_stories.items()):\n",
    "#     for j, (id2, desc2) in enumerate(processed_stories.items()):\n",
    "#         if i < j:  # to avoid repeating the same pairs\n",
    "#             print(f\"Similarity between Story {id1} and Story {id2}: {similarities[i][j]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to Run\n",
    "Install Required Libraries: Make sure you have nltk and sklearn installed in your Python environment. You can install them using pip:\n",
    "\n",
    "Copy code\n",
    "pip install nltk sklearn\n",
    "Expand the Dataset: You can add more user stories to the user_stories dictionary.\n",
    "\n",
    "Run the Script: This script extracts keywords, computes TF-IDF vectors, and then calculates the cosine similarity between each pair of user stories to find potential connections.\n",
    "\n",
    "This approach provides a basic framework. You might need to refine the keyword extraction and similarity thresholds based on your specific requirements or use more sophisticated NLP models for better accuracy. If you need more advanced analysis or help with a specific part of the code, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternativ: \n",
    "    - https://dylancastillo.co/clustering-documents-with-openai-langchain-hdbscan/ (Simple Clusterin with just Word Embeddings and k-meas algorithm)\n",
    "    - https://github.com/daniel-furman/awesome-chatgpt-prompts-clustering (Simpler Clustering)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
